{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "# pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_intervals_multi(line):\n",
    "    \"\"\"\n",
    "    For a given line, return a dictionary where each key is a token (a contiguous\n",
    "    sequence of characters that are not separated by two or more spaces) and the\n",
    "    value is a tuple (start, end) representing the 1-indexed positions of the token in the line.\n",
    "    \n",
    "    Example:\n",
    "      Given the line: \"Age (yr)  Depth (cm)\"\n",
    "      This function returns:\n",
    "         {\"Age (yr)\": (1, 8), \"Depth (cm)\": (11, 21)}\n",
    "    \"\"\"\n",
    "    tokens = {}\n",
    "    # Split the line on sequences of two or more spaces.\n",
    "    # The capturing group ensures we retain the delimiters in the split result.\n",
    "    parts = re.split(r'(\\s{2,})', line)\n",
    "    pos = 0\n",
    "    for part in parts:\n",
    "        if re.fullmatch(r'\\s{2,}', part):\n",
    "            # Delimiter: update position by length of delimiter.\n",
    "            pos += len(part)\n",
    "        else:\n",
    "            if part:\n",
    "                # Record token: starting at pos+1 (1-indexed) and ending at pos+len(part)\n",
    "                start = pos + 1\n",
    "                end = pos + len(part)\n",
    "                tokens[part] = (start, end)\n",
    "                pos += len(part)\n",
    "    return tokens\n",
    "\n",
    "def get_token_intervals_multi_2(line):\n",
    "    \"\"\"\n",
    "    For a given line, return a dictionary where each key is a token (a contiguous\n",
    "    sequence of characters that are not separated by two or more spaces) and the\n",
    "    value is a tuple (start, end) representing the 1-indexed positions of the token in the line.\n",
    "    \n",
    "    If a token appears more than once, subsequent occurrences are suffixed with a counter.\n",
    "    \n",
    "    Example:\n",
    "      Given the line: \n",
    "        \"Age (yr)  Depth (cm)  Age (yr)  corrected  corrected  corrected\"\n",
    "      This function returns:\n",
    "         {\n",
    "           \"Age (yr)\": (1, 8),\n",
    "           \"Depth (cm)\": (11, 21),\n",
    "           \"Age (yr) 2\": (23, 30),\n",
    "           \"corrected\": (32, 40),\n",
    "           \"corrected 2\": (42, 50),\n",
    "           \"corrected 3\": (52, 60)\n",
    "         }\n",
    "    \"\"\"\n",
    "    tokens = {}\n",
    "    token_counts = {}  # To keep track of occurrences of each token\n",
    "    parts = re.split(r'(\\s{2,})', line)\n",
    "    pos = 0\n",
    "    for part in parts:\n",
    "        if re.fullmatch(r'\\s{2,}', part):\n",
    "            # If this part is a delimiter (2 or more spaces), update the current position.\n",
    "            pos += len(part)\n",
    "        else:\n",
    "            if part:\n",
    "                start = pos + 1\n",
    "                end = pos + len(part)\n",
    "                # Check if token already exists.\n",
    "                if part in token_counts:\n",
    "                    token_counts[part] += 1\n",
    "                    token_key = f\"{part} {token_counts[part]}\"\n",
    "                else:\n",
    "                    token_counts[part] = 1\n",
    "                    token_key = part\n",
    "                tokens[token_key] = (start, end)\n",
    "                pos += len(part)\n",
    "    return tokens\n",
    "\n",
    "# --- Example usage ---\n",
    "sample_text = \"\"\"Age (yr)  Depth (cm)  Age (yr)  corrected  corrected  corrected\"\"\"\n",
    "tokens = get_token_intervals_multi(sample_text)\n",
    "for token, interval in tokens.items():\n",
    "    print(f\"Token: '{token}', Interval: {interval}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interval_overlap(interval1, interval2):\n",
    "    \"\"\"\n",
    "    Given two intervals (a, b) and (c, d) (inclusive, 1-indexed),\n",
    "    compute the raw overlap (number of overlapping characters) and the Jaccard similarity.\n",
    "    Jaccard similarity = (size of intersection) / (size of union)\n",
    "    \"\"\"\n",
    "    a, b = interval1\n",
    "    c, d = interval2\n",
    "    start = max(a, c)\n",
    "    end = min(b, d)\n",
    "    raw_overlap = max(0, end - start + 1)\n",
    "    len1 = b - a + 1\n",
    "    len2 = d - c + 1\n",
    "    union = len1 + len2 - raw_overlap\n",
    "    jaccard = raw_overlap / union if union else 0\n",
    "    return raw_overlap, jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segregate_blocks(lines):\n",
    "    \"\"\"\n",
    "    Given a list of lines, group them into blocks separated by empty lines.\n",
    "    Each block is a list of non-empty lines.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    current_block = []\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            current_block.append(line)\n",
    "        else:\n",
    "            if current_block:\n",
    "                blocks.append(current_block)\n",
    "                current_block = []\n",
    "    if current_block:\n",
    "        blocks.append(current_block)\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_intervals_overlaps(file_path):\n",
    "    \"\"\"\n",
    "    Reads the file from file_path, segregates it into blocks (separated by empty lines),\n",
    "    and for each block computes token intervals (using a multi-space delimiter) and the\n",
    "    overlaps between tokens in the header (first line) and each data line.\n",
    "    \n",
    "    Returns:\n",
    "      A list of DataFrames—one per block—with the following columns:\n",
    "         - Token, Start Range, End Range, Overlap With, Raw Score, Jaccard Score, Line\n",
    "      For the header row, the last three columns are null.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        raw_data = file.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "        file_text = raw_data.decode(encoding)\n",
    "    \n",
    "    lines = file_text.splitlines()\n",
    "    blocks = segregate_blocks(lines)\n",
    "    block_dataframes = []\n",
    "    \n",
    "    print(len(blocks))\n",
    "    for block_idx, block in enumerate(blocks, start=1):\n",
    "        print(f\"--- Processing Block {block_idx} ---\")\n",
    "        line_tokens = [get_token_intervals_multi_2(line) for line in block]\n",
    "    \n",
    "        rows = []\n",
    "\n",
    "        header_tokens = line_tokens[0]\n",
    "        for token, interval in header_tokens.items():\n",
    "            rows.append({\n",
    "                \"Token\": token,\n",
    "                \"Start Range\": interval[0],\n",
    "                \"End Range\": interval[1],\n",
    "                \"Overlaps With\": None,\n",
    "                \"Raw Score\": None,\n",
    "                \"Jaccard Score\": None,\n",
    "                \"Line\": 1\n",
    "            })\n",
    "\n",
    "        for line_number, token_dict in enumerate(line_tokens[1:], start=2):\n",
    "            for token, interval in token_dict.items():\n",
    "                overlaps = [] \n",
    "                for header_token, header_interval in header_tokens.items():\n",
    "                    raw, jaccard = compute_interval_overlap(interval, header_interval)\n",
    "                    if raw > 0:\n",
    "                        overlaps.append((header_token, raw, jaccard))\n",
    "                if overlaps:\n",
    "                    for header_token, raw, jaccard in overlaps:\n",
    "                        rows.append({\n",
    "                            \"Token\": token,\n",
    "                            \"Start Range\": interval[0],\n",
    "                            \"End Range\": interval[1],\n",
    "                            \"Overlaps With\": header_token,\n",
    "                            \"Raw Score\": raw,\n",
    "                            \"Jaccard Score\": round(jaccard, 2),\n",
    "                            \"Line\": line_number\n",
    "                        })\n",
    "                else:\n",
    "                    rows.append({\n",
    "                        \"Token\": token,\n",
    "                        \"Start Range\": interval[0],\n",
    "                        \"End Range\": interval[1],\n",
    "                        \"Overlaps With\": None,\n",
    "                        \"Raw Score\": None,\n",
    "                        \"Jaccard Score\": None,\n",
    "                        \"Line\": line_number\n",
    "                    })\n",
    "        df_block = pd.DataFrame(rows)\n",
    "        block_dataframes.append(df_block)\n",
    "        display(df_block.head(len(header_tokens)*5)) \n",
    "        print(\"\\n========================\\n\")\n",
    "    \n",
    "    return block_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"./test_interval.txt\"\n",
    "    print(\"Displaying analysis of tokens overlaps.\\n\")\n",
    "    dfs = parse_intervals_overlaps(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
